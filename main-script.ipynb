{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "from IPython.display import display  # to display variables in a \"nice\" way\n",
    "\n",
    "# pd.options.display.max_rows = 9999\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Random State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs =  [312920,312919,313385]\n",
    "rs = np.min(IDs)\n",
    "np.random.seed(rs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 (Loading and Preparing the Data):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH TO THE cla4lsp22_bikez_curated.csv FILE\n",
    "bikez_path = 'cla4lsp22_bikez_curated.csv'\n",
    "\n",
    "# LOADING THE DATASET AS DATAFRAME then store in the variable df_tot\n",
    "df_tot = pd.read_csv(bikez_path)\n",
    "\n",
    "# DISPLAY OF THE DATAFRAME\n",
    "# display(df_tot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Generate x as a random number: 0, 1, or 2. workdf is the dftot containing only data corresponding to years with reminder r resulted by modulus 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = int(np.random.uniform(0,3))\n",
    "workdf = df_tot[df_tot['Year'] % 3 == x]\n",
    "display(workdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Remove randomly from workdf two columns among the features: Front/Rear breaks, Front/Rear tire, Front/Rear suspension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_features = ['Front brakes', 'Rear brakes','Front tire', 'Rear tire','Front suspension', 'Rear suspension']\n",
    "\n",
    "feat1,feat2 = np.random.choice(temp_features, 2, replace=False)\n",
    "workdf = workdf.drop(columns=[feat1,feat2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote:\n",
    "labels: the columns Brand, Model, Year, Category, Rating;\n",
    "features: all the other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = workdf.columns[:5].tolist()\n",
    "features = workdf.columns[5:].tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Clean the dataset workdf from missing values in the feature columns (if needed)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show percentage of missing values for each column of workdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the percentage of NaN values for each column\n",
    "na_percentage = workdf.isna().mean() * 100\n",
    "display(na_percentage)\n",
    "\n",
    "# filter the na_percentage series to only include columns with a percentage of NaN values > 0\n",
    "na_percentage_filtered = na_percentage[na_percentage > 0]\n",
    "\n",
    "# subtract the labels list from the list of columns with NaN values > 0\n",
    "na_features = na_percentage_filtered.index.difference(labels).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Nan values from 'Displacement (ccm)' since NaN values are sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdf.dropna(subset=['Displacement (ccm)'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Nan values from other columns by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in na_features:\n",
    "    workdf[col] = workdf[col].fillna(workdf[col].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 (Encoding of Categorical Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the string columns in workdf that are in the features list\n",
    "string_cols = workdf.select_dtypes(include=['object']).columns.intersection(features)\n",
    "\n",
    "# apply one-hot encoding to the selected string columns\n",
    "for col in string_cols:\n",
    "    encoded_cols = workdf[col].str.get_dummies(sep='.').add_prefix(col + '_')\n",
    "    workdf = pd.concat([workdf, encoded_cols], axis=1)\n",
    "\n",
    "    # drop the original string column\n",
    "    workdf.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# print the updated DataFrame\n",
    "display(workdf)\n",
    "\n",
    "# create a new DataFrame without the labels (only features)\n",
    "Xworkdf = workdf.drop(columns=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 (Preprocessing and PCA): Preprocess the data, before applying the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "Xworkdf_std = pd.DataFrame(scaler.fit_transform(Xworkdf), columns=Xworkdf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "Xworkdf_mm = pd.DataFrame(scaler.fit_transform(Xworkdf), columns=Xworkdf.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the variances of the original dataset Xworkdf with the variances of the datasets Xworkdf_std and Xworkdf_mm, we can calculate the variances of the non-categorical features in all three datasets.\n",
    "\n",
    "Assuming that Xworkdf contains both categorical and non-categorical features, we can select only the non-categorical features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncat_features = Xworkdf.select_dtypes(exclude='object').columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can calculate the variances of the non-categorical features in all three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variances of non-categorical features in Xworkdf\n",
    "variances_original = Xworkdf[noncat_features].var()\n",
    "\n",
    "# Calculate variances of non-categorical features in Xworkdf_std\n",
    "variances_standardized = Xworkdf_std[noncat_features].var()\n",
    "\n",
    "# Calculate variances of non-categorical features in Xworkdf_mm\n",
    "variances_minmax = Xworkdf_mm[noncat_features].var()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the variances of the non-categorical features in the three dataframes. We can plot the variances using a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Display only the initial 20 characteristics for the sake of legibility as the features are extensive and displaying all of them would be unsuitable.\n",
    "plt.bar(variances_original[:20].index, variances_original[:20], label='Original')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Variances of the original data')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Display only the initial 20 characteristics for the sake of legibility as the features are extensive and displaying all of them would be unsuitable.\n",
    "plt.bar(variances_standardized[:20].index, variances_standardized[:20], label='Standardized')\n",
    "plt.bar(variances_minmax[:20].index, variances_minmax[:20], label='Min-Max Scaled')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Comparison of Variances')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two plot with three bars, one for each dataset, with the height of each bar representing the variance of the corresponding non-categorical feature.\n",
    "\n",
    "From the plots, we can observe that the variances of the non-categorical features are different in the three datasets. Specifically, the variances of the standardized dataset Xworkdf_std are all equal to 1, as expected since this is a property of standardized data. On the other hand, the variances of the min-max scaled dataset Xworkdf_mm are all between 0 and 1, since this is the range specified by the MinMaxScaler. The variances of the original dataset Xworkdf are not normalized, and can be much larger than 1.\n",
    "\n",
    "Based on this analysis, we can infer that scaling the non-categorical features using either a StandardScaler or a MinMaxScaler can help to ensure that these features are on the same scale and have comparable variances. This can be particularly important for certain machine learning algorithms, such as those that rely on distance-based calculations or regularization, where features with large variances can have a disproportionate impact on the algorithm's performance. However, the choice of scaler may depend on the specific requirements of the problem at hand, and in some cases it may be appropriate to use a different scaler or no scaler at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the PCA class from the sklearn.decomposition module to fit a PCA model to each of the three DataFrames, with n_components='mle' to retain all components. We then calculate the cumulative explained variance using the explained_variance_ratio_ attribute of the PCA object, and store the results in three arrays cumulative_variances_original, cumulative_variances_standardized, and cumulative_variances_minmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object with all components\n",
    "pca = PCA(n_components='mle')\n",
    "\n",
    "# Fit the PCA model to the original data\n",
    "pca.fit(Xworkdf)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_original = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Fit the PCA model to the standardized data\n",
    "pca.fit(Xworkdf_std)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_standardized = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Fit the PCA model to the min-max scaled data\n",
    "pca.fit(Xworkdf_mm)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_minmax = np.cumsum(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_variances_original, label='Original')\n",
    "plt.plot(cumulative_variances_standardized, label='Standardized')\n",
    "plt.plot(cumulative_variances_minmax, label='Min-Max Scaled')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance by Number of Components')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a plot with three lines, one for each DataFrame, with the x-axis representing the number of components and the y-axis representing the cumulative explained variance.\n",
    "\n",
    "Looking at the results, we can see that in all three cases, the cumulative explained variance increases as the number of components increases, with the largest increase occurring in the first few components. The standardized dataset Xworkdf_std and the min-max scaled dataset Xworkdf_mm both have a more linear increase in cumulative explained variance than the original dataset Xworkdf, indicating that the PCA has reduced the impact of features with large variances.\n",
    "\n",
    "Compared to the previous analysis, we can now see that applying PCA has a significant impact on the amount of variance explained by the non-categorical features. In particular, we can observe that a small number of principal components explain a large amount of the variance in the standardized and min-max scaled datasets, suggesting that these scalers may be effective in reducing the impact of features with large variances. On the other hand, the original dataset has a more gradual increase in cumulative explained variance, indicating that the non-categorical features in this dataset may have a wider range of variances. Depending on the specific requirements of the problem, it may be necessary to use a scaler or feature selection technique to address this issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82deb7ebdf62c12271930ded557c7a095953f7973d7c91074b3cf1a3c24aef9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
