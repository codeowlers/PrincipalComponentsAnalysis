{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "from IPython.display import display  # to display variables in a \"nice\" way\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# pd.options.display.max_rows = 9999\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Random State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs =  [312920,312919,313385]\n",
    "rs = np.min(IDs)\n",
    "np.random.seed(rs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 (Loading and Preparing the Data):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH TO THE cla4lsp22_bikez_curated.csv FILE\n",
    "bikez_path = 'cla4lsp22_bikez_curated.csv'\n",
    "\n",
    "# LOADING THE DATASET AS DATAFRAME then store in the variable df_tot\n",
    "df_tot = pd.read_csv(bikez_path)\n",
    "\n",
    "# DISPLAY OF THE DATAFRAME\n",
    "# display(df_tot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Generate x as a random number: 0, 1, or 2. workdf is the dftot containing only data corresponding to years with reminder r resulted by modulus 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = int(np.random.uniform(0,3))\n",
    "workdf = df_tot[df_tot['Year'] % 3 == x]\n",
    "display(workdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Remove randomly from workdf two columns among the features: Front/Rear breaks, Front/Rear tire, Front/Rear suspension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_features = ['Front brakes', 'Rear brakes','Front tire', 'Rear tire','Front suspension', 'Rear suspension']\n",
    "\n",
    "feat1,feat2 = np.random.choice(temp_features, 2, replace=False)\n",
    "workdf = workdf.drop(columns=[feat1,feat2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote:\n",
    "labels: the columns Brand, Model, Year, Category, Rating;\n",
    "features: all the other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = workdf.columns[:5].tolist()\n",
    "features = workdf.columns[5:].tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Clean the dataset workdf from missing values in the feature columns (if needed)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show percentage of missing values for each column of workdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the percentage of NaN values for each column\n",
    "na_percentage = workdf.isna().mean() * 100\n",
    "display(na_percentage)\n",
    "\n",
    "# filter the na_percentage series to only include columns with a percentage of NaN values > 0\n",
    "na_percentage_filtered = na_percentage[na_percentage > 0]\n",
    "\n",
    "# subtract the labels list from the list of columns with NaN values > 0\n",
    "na_features = na_percentage_filtered.index.difference(labels).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Nan values from 'Displacement (ccm)' since NaN values are sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdf.dropna(subset=['Displacement (ccm)'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Nan values from other columns by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in na_features:\n",
    "    workdf[col] = workdf[col].fillna(workdf[col].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 (Encoding of Categorical Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the string columns in workdf that are in the features list\n",
    "string_cols = workdf.select_dtypes(include=['object']).columns.intersection(features)\n",
    "\n",
    "# apply one-hot encoding to the selected string columns\n",
    "for col in string_cols:\n",
    "    encoded_cols = workdf[col].str.get_dummies(sep='.').add_prefix(col + '_')\n",
    "    workdf = pd.concat([workdf, encoded_cols], axis=1)\n",
    "\n",
    "    # drop the original string column\n",
    "    workdf.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# create a new DataFrame without the labels (only features)\n",
    "Xworkdf = workdf.drop(columns=labels)\n",
    "\n",
    "# print the updated DataFrame\n",
    "display(Xworkdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 (Preprocessing and PCA): Preprocess the data, before applying the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "Xworkdf_std = pd.DataFrame(scaler.fit_transform(Xworkdf), columns=Xworkdf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "Xworkdf_mm = pd.DataFrame(scaler.fit_transform(Xworkdf), columns=Xworkdf.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the variances of the original dataset Xworkdf with the variances of the datasets Xworkdf_std and Xworkdf_mm, we can calculate the variances of the non-categorical features in all three datasets.\n",
    "\n",
    "Assuming that Xworkdf contains both categorical and non-categorical features, we can select only the non-categorical features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncat_features = Xworkdf.select_dtypes(exclude='object').columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can calculate the variances of the non-categorical features in all three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variances of non-categorical features in Xworkdf\n",
    "variances_original = Xworkdf[noncat_features].var()\n",
    "\n",
    "# Calculate variances of non-categorical features in Xworkdf_std\n",
    "variances_standardized = Xworkdf_std[noncat_features].var()\n",
    "\n",
    "# Calculate variances of non-categorical features in Xworkdf_mm\n",
    "variances_minmax = Xworkdf_mm[noncat_features].var()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the variances of the non-categorical features in the three dataframes. We can plot the variances using a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Display only the initial 20 characteristics for the sake of legibility as the features are extensive and displaying all of them would be unsuitable.\n",
    "plt.bar(variances_original[:20].index, variances_original[:20], label='Original')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Variances of the original data')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Display only the initial 20 characteristics for the sake of legibility as the features are extensive and displaying all of them would be unsuitable.\n",
    "plt.bar(variances_standardized[:20].index, variances_standardized[:20], label='Standardized')\n",
    "plt.bar(variances_minmax[:20].index, variances_minmax[:20], label='Min-Max Scaled')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Comparison of Variances')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two plot with three bars, one for each dataset, with the height of each bar representing the variance of the corresponding non-categorical feature.\n",
    "\n",
    "From the plots, we can observe that the variances of the non-categorical features are different in the three datasets. Specifically, the variances of the standardized dataset Xworkdf_std are all equal to 1, as expected since this is a property of standardized data. On the other hand, the variances of the min-max scaled dataset Xworkdf_mm are all between 0 and 1, since this is the range specified by the MinMaxScaler. The variances of the original dataset Xworkdf are not normalized, and can be much larger than 1.\n",
    "\n",
    "Based on this analysis, we can infer that scaling the non-categorical features using either a StandardScaler or a MinMaxScaler can help to ensure that these features are on the same scale and have comparable variances. This can be particularly important for certain machine learning algorithms, such as those that rely on distance-based calculations or regularization, where features with large variances can have a disproportionate impact on the algorithm's performance. However, the choice of scaler may depend on the specific requirements of the problem at hand, and in some cases it may be appropriate to use a different scaler or no scaler at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we use the PCA class from the sklearn.decomposition module to fit a PCA model to each of the three DataFrames, with n_components='mle' to retain all components. We then calculate the cumulative explained variance using the explained_variance_ratio_ attribute of the PCA object, and store the results in three arrays cumulative_variances_original, cumulative_variances_standardized, and cumulative_variances_minmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object with all components\n",
    "pca = PCA(n_components='mle')\n",
    "\n",
    "# Fit the PCA model to the original data\n",
    "pca.fit(Xworkdf)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_original = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Fit the PCA model to the standardized data\n",
    "pca.fit(Xworkdf_std)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_standardized = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Fit the PCA model to the min-max scaled data\n",
    "pca.fit(Xworkdf_mm)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_minmax = np.cumsum(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_variances_original, label='Original')\n",
    "plt.plot(cumulative_variances_standardized, label='Standardized')\n",
    "plt.plot(cumulative_variances_minmax, label='Min-Max Scaled')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance by Number of Components')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a plot with three lines, one for each DataFrame, with the x-axis representing the number of components and the y-axis representing the cumulative explained variance.\n",
    "\n",
    "The original dataset, Xworkdf, has a constant cumulative explained variance of 1, which means that all of the variance in the data is explained by the full set of features. This is expected, as the original dataset has not been transformed or scaled in any way.\n",
    "\n",
    "The standardized dataset, Xworkdf_std, has a slower increase in cumulative explained variance compared to the min-max scaled dataset, Xworkdf_mm, but still reaches a cumulative variance of 0.9 after approximately 85 principal components and then it reached a cumulative variance of 1 after 110 components. This indicates that standardizing the dataset has reduced the impact of features with large variances, but there are still a large number of principal components required to explain the majority of the variance in the data.\n",
    "\n",
    "In contrast, the min-max scaled dataset, Xworkdf_mm, has a much faster increase in cumulative explained variance and reaches a cumulative variance of 0.9 after only 25 principal components and then it reached 1 for the 65 components. This suggests that scaling the dataset to a specific range has had a significant impact on reducing the impact of features with large variances, and the majority of the variance in the data can be explained by a smaller number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA on Xworkdf_std\n",
    "pca_std = PCA().fit(Xworkdf_std)\n",
    "# cumulative sum of explained variance\n",
    "cum_var_std = np.cumsum(pca_std.explained_variance_ratio_)\n",
    "# minimum number of PCs to explain 35% of variance\n",
    "min_pc_std = np.argmax(cum_var_std >= 0.35) + 1\n",
    "# number of PCs to select\n",
    "num_pc_std = min(min_pc_std, 5)\n",
    "# fit PCA with selected number of PCs\n",
    "pca_std = PCA(n_components=num_pc_std).fit(Xworkdf_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA on Xworkdf_mm\n",
    "pca_mm = PCA().fit(Xworkdf_mm)\n",
    "# cumulative sum of explained variance\n",
    "cum_var_mm = np.cumsum(pca_mm.explained_variance_ratio_)\n",
    "# minimum number of PCs to explain 35% of variance\n",
    "min_pc_mm = np.argmax(cum_var_mm >= 0.35) + 1\n",
    "# number of PCs to select\n",
    "num_pc_mm = min(min_pc_mm, 5)\n",
    "# fit PCA with selected number of PCs\n",
    "pca_mm = PCA(n_components=num_pc_mm).fit(Xworkdf_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot barplots of percentage of explained variance\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].bar(range(1, num_pc_std+1), pca_std.explained_variance_ratio_)\n",
    "axs[0].set_title('Xworkdf_std')\n",
    "axs[0].set_xlabel('PC')\n",
    "axs[0].set_ylabel('Explained Variance Ratio')\n",
    "axs[1].bar(range(1, num_pc_mm+1), pca_mm.explained_variance_ratio_)\n",
    "axs[1].set_title('Xworkdf_mm')\n",
    "axs[1].set_xlabel('PC')\n",
    "axs[1].set_ylabel('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This code first applies PCA to both dataframes using the default number of components. It then calculates the cumulative sum of explained variance and determines the minimum number of PCs required to explain 35% of the total variance. The code then selects the minimum number of PCs or 5, whichever is smaller. Finally, it fits PCA with the selected number of PCs and plots the barplots of the percentage of explained variance for both dataframes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the PCs - Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the original features\n",
    "feature_names = Xworkdf.columns.tolist()\n",
    "\n",
    "# Get the loadings for each principal component\n",
    "loadings = pca_std.components_\n",
    "\n",
    "# Get the threshold\n",
    "eps_std = np.sqrt(1 / pca_std.n_features_)\n",
    "# Loop through each principal component and print the top 5 features with the highest positive and negative loadings\n",
    "for i, pc in enumerate(loadings):\n",
    "    print(f\"PC{i+1}:\")\n",
    "    top_pos = [j for j in range(len(pc)) if pc[j] > eps_std][:5]\n",
    "    top_neg = [j for j in range(len(pc)) if pc[j] < -eps_std][:5]\n",
    "    for j in range(len(top_pos)):\n",
    "        print(f\"\\t{feature_names[top_pos[j]]}: {pc[top_pos[j]]:.2f}\")\n",
    "    for j in range(len(top_neg)):\n",
    "        print(f\"\\t{feature_names[top_neg[j]]}: {pc[top_neg[j]]:.2f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name for each PC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST OF THE NAMES ASSIGNED TO THE PCs - Standard Scaler\n",
    "pc_std_names = ['Engine size and type',\n",
    "           'Transmission and engine type',\n",
    "           'Fuel system and engine type',\n",
    "           'Weight and engine type',\n",
    "           'Engine type and gearbox'\n",
    "           ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1: This component is mainly characterized by features related to the engine size and type, with displacement, power, torque, bore and stroke having the highest positive loadings. On the other hand, features related to the engine's cooling and fuel systems have the highest negative loadings. This component represents the overall size and power of the engine.\n",
    "\n",
    "PC2: This component is mainly characterized by features related to the transmission and engine type, with engine cylinder type and stroke having the highest positive loadings, and gearbox type having the highest negative loading. This component represents the type of transmission and engine used in the motorcycle.\n",
    "\n",
    "PC3:  This component is mainly characterized by features related to the fuel system and engine type, with fuel control, fuel system, engine cylinder type and stroke having the highest positive loadings, and displacement and stroke having the highest negative loadings. This component represents the type of fuel system and engine used in the motorcycle.\n",
    "\n",
    "PC4: This component is mainly characterized by features related to the weight and engine type, with stroke, dry weight, and engine cylinder type having the highest positive loadings, and power and seat height having the highest negative loadings. This component represents the weight and type of engine used in the motorcycle.\n",
    "\n",
    "PC5: This component is mainly characterized by features related to the engine type and gearbox, with engine cylinder type, engine stroke, and gearbox type having the highest positive loadings, and fuel control and gearbox having the highest negative loadings. This component represents the type of engine and gearbox used in the motorcycle.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the PCs - MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the loadings for each principal component\n",
    "loadings = pca_mm.components_\n",
    "\n",
    "# Get the threshold\n",
    "eps_mm = np.sqrt(1 / pca_mm.n_features_)\n",
    "\n",
    "# Loop through each principal component and print the top 5 features with the highest positive and negative loadings\n",
    "for i, pc in enumerate(loadings):\n",
    "    print(f\"PC{i+1}:\")\n",
    "    top_pos = [j for j in range(len(pc)) if pc[j] > eps_mm][:5]\n",
    "    top_neg = [j for j in range(len(pc)) if pc[j] < -eps_mm][:5]\n",
    "    for j in range(len(top_pos)):\n",
    "        print(f\"\\t{feature_names[top_pos[j]]}: {pc[top_pos[j]]:.2f}\")\n",
    "    for j in range(len(top_neg)):\n",
    "        print(f\"\\t{feature_names[top_neg[j]]}: {pc[top_neg[j]]:.2f}\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name for each PC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIST OF THE NAMES ASSIGNED TO THE PCs - MinMax\n",
    "pc_mm_names = ['Engine and Transmission Type',\n",
    "           'Cylinder Type and Fuel Control',\n",
    "           'Engine Stroke and Cooling System',\n",
    "           'Gearbox and Engine Cylinder'\n",
    "\n",
    "           ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC1: Engine and Transmission Type\n",
    "This component is mainly characterized by engine and transmission type features. It represents the difference between four-stroke and two-stroke engines, the use of a 6-speed gearbox, the preference for liquid cooling systems, and the fuel system type. On the other hand, it also shows a negative correlation with air cooling systems, unknown fuel control, and unknown gearbox types.\n",
    "\n",
    "PC2: Cylinder Type and Fuel Control\n",
    "This component is mainly characterized by cylinder type and fuel control features. It represents the difference between single-cylinder and in-line four engines, the preference for two-stroke engines, and automatic gearboxes. It also shows a positive correlation with unknown fuel control and single overhead cam systems, while it has a negative correlation with V2 engines, four-stroke engines, 5-speed gearboxes, and double overhead cam systems.\n",
    "\n",
    "PC3: Engine Stroke and Cooling System\n",
    "This component is mainly characterized by engine stroke and cooling system features. It represents the difference between two-stroke and four-stroke engines, the preference for 6-speed gearboxes, the use of liquid cooling systems, and the fuel control type. It also shows a positive correlation with single-cylinder engines, while it has a negative correlation with V2 engines, overhead valves, single overhead cam systems, and air cooling systems.\n",
    "\n",
    "PC4: Gearbox and Engine Cylinder\n",
    "This component is mainly characterized by gearbox and engine cylinder features. It represents the difference between automatic and 5-speed gearboxes, the preference for V2 engines, four-stroke engines, and single overhead cam systems. On the other hand, it also shows a negative correlation with twin engines, two-stroke engines, 6-speed gearboxes, and double overhead cam systems. Additionally, it is strongly correlated with chain transmissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE THE DATA TRANSFORMATION INTO THE PC-SPACE\n",
    "\n",
    "Yworkdf_std = pca_std.transform(Xworkdf_std)\n",
    "\n",
    "Yworkdf_mm = pca_mm.transform(Xworkdf_mm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# get the scores for the first num_dims PCs for both dataframes\n",
    "if num_pc_std == 2:\n",
    "    dims_std = 2\n",
    "elif num_pc_std >= 3:\n",
    "    dims_std = 3\n",
    "\n",
    "stds = workdf['Category'].unique()\n",
    "std_cmap = cm.tab20(np.linspace(0, 1, len(stds)))\n",
    "std_colors = {stds[i]: std_cmap[i] for i in range(len(stds))}\n",
    "Yworkdf_std = pca_std.transform(Xworkdf_std)[:, :dims_std]\n",
    "workdf['color'] = [std_colors[t] for t in workdf['Category'].values]\n",
    "\n",
    "# plot the scores for Xworkdf_std\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "if dims_std == 2:\n",
    "    ax.scatter(Yworkdf_std[:, 0], Yworkdf_std[:, 1], c=workdf['color'], s=0.5)\n",
    "    ax.set_xlabel(f'{pc_std_names[0]}')\n",
    "    ax.set_ylabel(f'{pc_std_names[1]} ')\n",
    "else:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(Yworkdf_std[:, 0], Yworkdf_std[:, 1], Yworkdf_std[:, 2], c=workdf['color'], s=0.5)\n",
    "    ax.set_xlabel(f'{pc_std_names[0]}')\n",
    "    ax.set_ylabel(f'{pc_std_names[1]}')\n",
    "    ax.set_zlabel(f'{pc_std_names[2]}')\n",
    "ax.set_title('Score plot for Xworkdf_std')\n",
    "handles = []\n",
    "labels = []\n",
    "for category, color in std_colors.items():\n",
    "    handles.append(ax.scatter([], [], color=color))\n",
    "    labels.append(category)\n",
    "ax.legend(handles, labels, loc='upper right', title='Categories')\n",
    "plt.show()\n",
    "\n",
    "# plot the scores for Xworkdf_mm\n",
    "# Check number of dimensions\n",
    "if num_pc_mm == 2:\n",
    "    dims_mm = 2\n",
    "\n",
    "elif num_pc_mm >= 3:\n",
    "    dims_mm = 3\n",
    "\n",
    "scores_mm = pca_mm.transform(Xworkdf_mm)[:, :dims_mm]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "if dims_mm == 2:\n",
    "    ax.scatter(scores_mm[:, 0], scores_mm[:, 1], c=workdf['color'], s=1)\n",
    "    ax.set_xlabel(f'{pc_mm_names[0]}')\n",
    "    ax.set_ylabel(f'{pc_mm_names[1]}')\n",
    "else:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(scores_mm[:, 0], scores_mm[:, 1], scores_mm[:, 2], c=workdf['color'], s=1)\n",
    "    ax.set_xlabel(f'{pc_mm_names[0]}')\n",
    "    ax.set_ylabel(f'{pc_mm_names[1]}')\n",
    "    ax.set_zlabel(f'{pc_mm_names[2]}')\n",
    "ax.set_title('Score plot for Xworkdf_mm')\n",
    "ax.legend(handles, labels, loc='upper right', title='Categories')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 5 - (k-Means):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Define the range of k values to test\n",
    "k_values = range(3, 11)\n",
    "\n",
    "kmeans_std=None\n",
    "kmeans_mm=None\n",
    "\n",
    "# Iterate over each dataframe\n",
    "for i, df in enumerate([Yworkdf_std, Yworkdf_mm]):\n",
    "    print(f\"Results for DataFrame {i+1}:\\n\")\n",
    "    best_score = -1\n",
    "    best_k = None\n",
    "    for k in k_values:\n",
    "        # Fit the k-means model\n",
    "        model = KMeans(n_clusters=k, random_state=rs,n_init=3).fit(df)\n",
    "        \n",
    "        # Calculate the silhouette score\n",
    "        score = silhouette_score(df, model.labels_)\n",
    "        print(f\"k = {k}, silhouette score = {score:.4f}\")\n",
    "        # Update the best score and k value if necessary\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "            if i == 0:\n",
    "                kmeans_std = model\n",
    "            else:\n",
    "                kmeans_mm = model\n",
    "    \n",
    "    print(f\"Best value of k= {best_k}, silhouette score = {best_score:.4f} \\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 6 - (Clusters and Centroid Interpretation and Visualization): "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroid Interpretation (Standard Scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster labels and centroids\n",
    "labels_std = kmeans_std.labels_\n",
    "centroids_std = kmeans_std.cluster_centers_\n",
    "\n",
    "# Define a list of feature names\n",
    "feature_names = pc_std_names\n",
    "\n",
    "# Convert array to dataframe with column names\n",
    "Yworkdf_std_df = pd.DataFrame(Yworkdf_std, columns=[f\"{feature_names[i]}\" for i in range(5)])\n",
    "\n",
    "# Iterate over each cluster to interpret the centroids and assign names\n",
    "for i in range(centroids_std.shape[0]):\n",
    "    # Get the indices of the data points in the cluster\n",
    "    cluster_indices = np.where(labels_std == i)[0]\n",
    "    \n",
    "    # Get the mean values of the principal components for the cluster\n",
    "    mean_pc_values = np.mean(Yworkdf_std_df.iloc[cluster_indices], axis=0)\n",
    "    \n",
    "    # Find the index of the principal component with the highest absolute value\n",
    "    dominant_pc_index = np.argmax(np.abs(mean_pc_values))\n",
    "    \n",
    "    # Get the sign of the dominant principal component\n",
    "    dominant_pc_sign = np.sign(mean_pc_values[dominant_pc_index])\n",
    "    \n",
    "    # Get the name of the feature that corresponds to the dominant principal component\n",
    "    dominant_feature = feature_names[dominant_pc_index]\n",
    "    \n",
    "    # Assign a name to the cluster based on the dominant feature and sign\n",
    "    cluster_name = ''\n",
    "    if dominant_pc_sign > 0:\n",
    "        cluster_name += 'High '\n",
    "    else:\n",
    "        cluster_name += 'Low '\n",
    "    cluster_name += dominant_feature \n",
    "    \n",
    "    # Print the name and mean principal component values for the cluster\n",
    "    print(f'Cluster {i+1}: Average Motorcycle with {cluster_name}')\n",
    "    print(f'Mean PC values: {mean_pc_values}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroid Interpretation (Min Max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the cluster labels and centroids\n",
    "labels_mm = kmeans_mm.labels_\n",
    "centroids_mm = kmeans_mm.cluster_centers_\n",
    "\n",
    "# Define a list of feature names\n",
    "feature_names = pc_std_names\n",
    "\n",
    "# Convert array to dataframe with column names\n",
    "Yworkdf_std_df = pd.DataFrame(Yworkdf_std, columns=[f\"{feature_names[i]}\" for i in range(5)])\n",
    "\n",
    "# Iterate over each cluster to interpret the centroids and assign names\n",
    "for i in range(centroids_mm.shape[0]):\n",
    "    # Get the indices of the data points in the cluster\n",
    "    cluster_indices = np.where(labels_mm == i)[0]\n",
    "    \n",
    "    # Get the mean values of the principal components for the cluster\n",
    "    mean_pc_values = np.mean(Yworkdf_std_df.iloc[cluster_indices], axis=0)\n",
    "    \n",
    "    # Find the index of the principal component with the highest absolute value\n",
    "    dominant_pc_index = np.argmax(np.abs(mean_pc_values))\n",
    "    \n",
    "    # Get the sign of the dominant principal component\n",
    "    dominant_pc_sign = np.sign(mean_pc_values[dominant_pc_index])\n",
    "    \n",
    "    # Get the name of the feature that corresponds to the dominant principal component\n",
    "    dominant_feature = feature_names[dominant_pc_index]\n",
    "    \n",
    "    # Assign a name to the cluster based on the dominant feature and sign\n",
    "    cluster_name = ''\n",
    "    if dominant_pc_sign > 0:\n",
    "        cluster_name += 'High '\n",
    "    else:\n",
    "        cluster_name += 'Low '\n",
    "    cluster_name += dominant_feature \n",
    "    \n",
    "    # Print the name and mean principal component values for the cluster\n",
    "    print(f'Cluster {i+1}: Average Motorcycle with {cluster_name}')\n",
    "    print(f'Mean PC values: {mean_pc_values}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(Y, labels,centroids, num_pc, pc_names, title):\n",
    "   \n",
    "    \n",
    "    # get the category colors\n",
    "    categories = workdf['Category'].unique()\n",
    "    cmap = cm.tab20(np.linspace(0, 1, len(categories)))\n",
    "    colors = {categories[i]: cmap[i] for i in range(len(categories))}\n",
    "    \n",
    "    # add the color column to the data frame\n",
    "    workdf['color'] = [colors[t] for t in workdf['Category'].values]\n",
    "    \n",
    "    # create the plot\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if num_pc == 2:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], c=labels, cmap='tab20', s=0.5)\n",
    "        ax.set_xlabel(f'{pc_names[0]}')\n",
    "        ax.set_ylabel(f'{pc_names[1]}')\n",
    "    else:\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], c=labels, cmap='tab20', s=0.5)\n",
    "        ax.set_xlabel(f'{pc_names[0]}')\n",
    "        ax.set_ylabel(f'{pc_names[1]}')\n",
    "        ax.set_zlabel(f'{pc_names[2]}')\n",
    "    \n",
    "    \n",
    "     # add the centroids to the plot\n",
    "    if num_pc == 2:\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=100)\n",
    "    else:\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], c='black', marker='x', s=100, alpha=1)\n",
    "    \n",
    "   \n",
    "    # set the title and legend\n",
    "    ax.set_title(f'Score plot for {title}')\n",
    "    handles = []\n",
    "    labels = []\n",
    "    for category, color in colors.items():\n",
    "        handles.append(ax.scatter([], [], color=color))\n",
    "        labels.append(category)\n",
    "    ax.legend(handles, labels, loc='upper right', title='Categories')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# for Standard Scaler\n",
    "plot_scores(Yworkdf_std,labels_std,centroids_std,num_pc_std,pc_std_names,\"Standard Scaler\")\n",
    "\n",
    "# for MinMax Scaler\n",
    "plot_scores(Yworkdf_mm,labels_mm,centroids_mm,num_pc_mm,pc_mm_names,\"MinMax Scaler\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82deb7ebdf62c12271930ded557c7a095953f7973d7c91074b3cf1a3c24aef9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
