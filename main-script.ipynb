{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "from IPython.display import display  # to display variables in a \"nice\" way\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "# pd.options.display.max_rows = 9999\n",
    "pd.options.display.max_columns = 200"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the Random State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDs =  [312920,312919,313385]\n",
    "rs = np.min(IDs)\n",
    "np.random.seed(rs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 (Loading and Preparing the Data):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH TO THE cla4lsp22_bikez_curated.csv FILE\n",
    "bikez_path = 'cla4lsp22_bikez_curated.csv'\n",
    "\n",
    "# LOADING THE DATASET AS DATAFRAME then store in the variable df_tot\n",
    "df_tot = pd.read_csv(bikez_path)\n",
    "\n",
    "# DISPLAY OF THE DATAFRAME\n",
    "# display(df_tot)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Generate x as a random number: 0, 1, or 2. workdf is the dftot containing only data corresponding to years with reminder r resulted by modulus 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = int(np.random.uniform(0,3))\n",
    "workdf = df_tot[df_tot['Year'] % 3 == x]\n",
    "display(workdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Remove randomly from workdf two columns among the features: Front/Rear breaks, Front/Rear tire, Front/Rear suspension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_features = ['Front brakes', 'Rear brakes','Front tire', 'Rear tire','Front suspension', 'Rear suspension']\n",
    "\n",
    "feat1,feat2 = np.random.choice(temp_features, 2, replace=False)\n",
    "workdf = workdf.drop(columns=[feat1,feat2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote:\n",
    "labels: the columns Brand, Model, Year, Category, Rating;\n",
    "features: all the other ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = workdf.columns[:5].tolist()\n",
    "features = workdf.columns[5:].tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Clean the dataset workdf from missing values in the feature columns (if needed)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show percentage of missing values for each column of workdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the percentage of NaN values for each column\n",
    "na_percentage = workdf.isna().mean() * 100\n",
    "display(na_percentage)\n",
    "\n",
    "# filter the na_percentage series to only include columns with a percentage of NaN values > 0\n",
    "na_percentage_filtered = na_percentage[na_percentage > 0]\n",
    "\n",
    "# subtract the labels list from the list of columns with NaN values > 0\n",
    "na_features = na_percentage_filtered.index.difference(labels).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Nan values from 'Displacement (ccm)' since NaN values are sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workdf.dropna(subset=['Displacement (ccm)'], inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill Nan values from other columns by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in na_features:\n",
    "    workdf[col] = workdf[col].fillna(workdf[col].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 (Encoding of Categorical Data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the string columns in workdf that are in the features list\n",
    "string_cols = workdf.select_dtypes(include=['object']).columns.intersection(features)\n",
    "\n",
    "# apply one-hot encoding to the selected string columns\n",
    "for col in string_cols:\n",
    "    encoded_cols = workdf[col].str.get_dummies(sep='.').add_prefix(col + '_')\n",
    "    workdf = pd.concat([workdf, encoded_cols], axis=1)\n",
    "\n",
    "    # drop the original string column\n",
    "    workdf.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# print the updated DataFrame\n",
    "display(workdf)\n",
    "\n",
    "# create a new DataFrame without the labels (only features)\n",
    "Xworkdf = workdf.drop(columns=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 (Preprocessing and PCA): Preprocess the data, before applying the PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "Xworkdf_std = pd.DataFrame(scaler.fit_transform(Xworkdf), columns=Xworkdf.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data and transform the data\n",
    "Xworkdf_mm = pd.DataFrame(scaler.fit_transform(Xworkdf), columns=Xworkdf.columns)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the variances of the original dataset Xworkdf with the variances of the datasets Xworkdf_std and Xworkdf_mm, we can calculate the variances of the non-categorical features in all three datasets.\n",
    "\n",
    "Assuming that Xworkdf contains both categorical and non-categorical features, we can select only the non-categorical features as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncat_features = Xworkdf.select_dtypes(exclude='object').columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can calculate the variances of the non-categorical features in all three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variances of non-categorical features in Xworkdf\n",
    "variances_original = Xworkdf[noncat_features].var()\n",
    "\n",
    "# Calculate variances of non-categorical features in Xworkdf_std\n",
    "variances_standardized = Xworkdf_std[noncat_features].var()\n",
    "\n",
    "# Calculate variances of non-categorical features in Xworkdf_mm\n",
    "variances_minmax = Xworkdf_mm[noncat_features].var()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the variances of the non-categorical features in the three dataframes. We can plot the variances using a bar plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Display only the initial 20 characteristics for the sake of legibility as the features are extensive and displaying all of them would be unsuitable.\n",
    "plt.bar(variances_original[:20].index, variances_original[:20], label='Original')\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Variances of the original data')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Display only the initial 20 characteristics for the sake of legibility as the features are extensive and displaying all of them would be unsuitable.\n",
    "plt.bar(variances_standardized[:20].index, variances_standardized[:20], label='Standardized')\n",
    "plt.bar(variances_minmax[:20].index, variances_minmax[:20], label='Min-Max Scaled')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Comparison of Variances')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce two plot with three bars, one for each dataset, with the height of each bar representing the variance of the corresponding non-categorical feature.\n",
    "\n",
    "From the plots, we can observe that the variances of the non-categorical features are different in the three datasets. Specifically, the variances of the standardized dataset Xworkdf_std are all equal to 1, as expected since this is a property of standardized data. On the other hand, the variances of the min-max scaled dataset Xworkdf_mm are all between 0 and 1, since this is the range specified by the MinMaxScaler. The variances of the original dataset Xworkdf are not normalized, and can be much larger than 1.\n",
    "\n",
    "Based on this analysis, we can infer that scaling the non-categorical features using either a StandardScaler or a MinMaxScaler can help to ensure that these features are on the same scale and have comparable variances. This can be particularly important for certain machine learning algorithms, such as those that rely on distance-based calculations or regularization, where features with large variances can have a disproportionate impact on the algorithm's performance. However, the choice of scaler may depend on the specific requirements of the problem at hand, and in some cases it may be appropriate to use a different scaler or no scaler at all."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here we use the PCA class from the sklearn.decomposition module to fit a PCA model to each of the three DataFrames, with n_components='mle' to retain all components. We then calculate the cumulative explained variance using the explained_variance_ratio_ attribute of the PCA object, and store the results in three arrays cumulative_variances_original, cumulative_variances_standardized, and cumulative_variances_minmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a PCA object with all components\n",
    "pca = PCA(n_components='mle')\n",
    "\n",
    "# Fit the PCA model to the original data\n",
    "pca.fit(Xworkdf)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_original = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Fit the PCA model to the standardized data\n",
    "pca.fit(Xworkdf_std)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_standardized = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Fit the PCA model to the min-max scaled data\n",
    "pca.fit(Xworkdf_mm)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variances_minmax = np.cumsum(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_variances_original, label='Original')\n",
    "plt.plot(cumulative_variances_standardized, label='Standardized')\n",
    "plt.plot(cumulative_variances_minmax, label='Min-Max Scaled')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance by Number of Components')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will produce a plot with three lines, one for each DataFrame, with the x-axis representing the number of components and the y-axis representing the cumulative explained variance.\n",
    "\n",
    "The original dataset, Xworkdf, has a constant cumulative explained variance of 1, which means that all of the variance in the data is explained by the full set of features. This is expected, as the original dataset has not been transformed or scaled in any way.\n",
    "\n",
    "The standardized dataset, Xworkdf_std, has a slower increase in cumulative explained variance compared to the min-max scaled dataset, Xworkdf_mm, but still reaches a cumulative variance of 0.9 after approximately 85 principal components and then it reached a cumulative variance of 1 after 110 components. This indicates that standardizing the dataset has reduced the impact of features with large variances, but there are still a large number of principal components required to explain the majority of the variance in the data.\n",
    "\n",
    "In contrast, the min-max scaled dataset, Xworkdf_mm, has a much faster increase in cumulative explained variance and reaches a cumulative variance of 0.9 after only 25 principal components and then it reached 1 for the 65 components. This suggests that scaling the dataset to a specific range has had a significant impact on reducing the impact of features with large variances, and the majority of the variance in the data can be explained by a smaller number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA on Xworkdf_std\n",
    "pca_std = PCA().fit(Xworkdf_std)\n",
    "# cumulative sum of explained variance\n",
    "cum_var_std = np.cumsum(pca_std.explained_variance_ratio_)\n",
    "# minimum number of PCs to explain 35% of variance\n",
    "min_pc_std = np.argmax(cum_var_std >= 0.35) + 1\n",
    "# number of PCs to select\n",
    "num_pc_std = min(min_pc_std, 5)\n",
    "# fit PCA with selected number of PCs\n",
    "pca_std = PCA(n_components=num_pc_std).fit(Xworkdf_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA on Xworkdf_mm\n",
    "pca_mm = PCA().fit(Xworkdf_mm)\n",
    "# cumulative sum of explained variance\n",
    "cum_var_mm = np.cumsum(pca_mm.explained_variance_ratio_)\n",
    "# minimum number of PCs to explain 35% of variance\n",
    "min_pc_mm = np.argmax(cum_var_mm >= 0.35) + 1\n",
    "# number of PCs to select\n",
    "num_pc_mm = min(min_pc_mm, 5)\n",
    "# fit PCA with selected number of PCs\n",
    "pca_mm = PCA(n_components=num_pc_mm).fit(Xworkdf_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot barplots of percentage of explained variance\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axs[0].bar(range(1, num_pc_std+1), pca_std.explained_variance_ratio_)\n",
    "axs[0].set_title('Xworkdf_std')\n",
    "axs[0].set_xlabel('PC')\n",
    "axs[0].set_ylabel('Explained Variance Ratio')\n",
    "axs[1].bar(range(1, num_pc_mm+1), pca_mm.explained_variance_ratio_)\n",
    "axs[1].set_title('Xworkdf_mm')\n",
    "axs[1].set_xlabel('PC')\n",
    "axs[1].set_ylabel('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This code first applies PCA to both dataframes using the default number of components. It then calculates the cumulative sum of explained variance and determines the minimum number of PCs required to explain 35% of the total variance. The code then selects the minimum number of PCs or 5, whichever is smaller. Finally, it fits PCA with the selected number of PCs and plots the barplots of the percentage of explained variance for both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the scores for the first num_dims PCs for both dataframes\n",
    "if num_pc_std == 2:\n",
    "    dims_std = 2 \n",
    "elif num_pc_std >= 3:\n",
    "    dims_std = 3\n",
    "\n",
    "scores_std = pca_std.transform(Xworkdf_std)[:, :dims_std]\n",
    "\n",
    "# plot the scores for Xworkdf_std\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "if dims_std == 2:\n",
    "    ax.scatter(scores_std[:, 0], scores_std[:, 1])\n",
    "    ax.set_xlabel('PC1 ({:.2f})'.format(pca_std.explained_variance_ratio_[0]))\n",
    "    ax.set_ylabel('PC2 ({:.2f})'.format(pca_std.explained_variance_ratio_[1]))\n",
    "else:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(scores_std[:, 0], scores_std[:, 1], scores_std[:, 2])\n",
    "    ax.set_xlabel('PC1 ({:.2f})'.format(pca_std.explained_variance_ratio_[0]))\n",
    "    ax.set_ylabel('PC2 ({:.2f})'.format(pca_std.explained_variance_ratio_[1]))\n",
    "    ax.set_zlabel('PC3 ({:.2f})'.format(pca_std.explained_variance_ratio_[2]))\n",
    "ax.set_title('Score plot for Xworkdf_std')\n",
    "plt.show()\n",
    "\n",
    "# plot the scores for Xworkdf_mm\n",
    "# Check number of dimensions\n",
    "if num_pc_mm == 2:\n",
    "    dims_mm = 2 \n",
    "    \n",
    "elif num_pc_mm >= 3:\n",
    "    dims_mm = 3\n",
    "\n",
    "scores_mm = pca_mm.transform(Xworkdf_mm)[:, :dims_mm]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "if dims_mm == 2:\n",
    "    ax.scatter(scores_mm[:, 0], scores_mm[:, 1])\n",
    "    ax.set_xlabel('PC1 ({:.2f})'.format(pca_mm.explained_variance_ratio_[0]))\n",
    "    ax.set_ylabel('PC2 ({:.2f})'.format(pca_mm.explained_variance_ratio_[1]))\n",
    "else:\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(scores_mm[:, 0], scores_mm[:, 1], scores_mm[:, 2])\n",
    "    ax.set_xlabel('PC1 ({:.2f})'.format(pca_mm.explained_variance_ratio_[0]))\n",
    "    ax.set_ylabel('PC2 ({:.2f})'.format(pca_mm.explained_variance_ratio_[1]))\n",
    "    ax.set_zlabel('PC3 ({:.2f})'.format(pca_mm.explained_variance_ratio_[2]))\n",
    "ax.set_title('Score plot for Xworkdf_mm')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82deb7ebdf62c12271930ded557c7a095953f7973d7c91074b3cf1a3c24aef9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
